{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Courier New; color:#CCCCCC\">**Dutch Named Entity Recognition CRF**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install eli5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Load Data and Imports**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\jerez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import convert_BIO\n",
    "from ner_evaluation import *\n",
    "from feature_getter import Feature_getter\n",
    "import pycrfsuite\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "ned_train = conll2002.iob_sents('ned.train')\n",
    "ned_val = conll2002.iob_sents('ned.testa')\n",
    "ned_test = conll2002.iob_sents('ned.testb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Preprocessing Data**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_train_BIO = convert_BIO(ned_train)\n",
    "ned_val_BIO = convert_BIO(ned_val)\n",
    "ned_test_BIO = convert_BIO(ned_test)\n",
    "\n",
    "X_val_BIO = [[word[0] for word in sent] for sent in ned_val_BIO]\n",
    "y_val_BIO = [[word[1] for word in sent] for sent in ned_val_BIO]\n",
    "X_test_BIO = [[word[0] for word in sent] for sent in ned_test_BIO]\n",
    "y_test_BIO = [[word[1] for word in sent] for sent in ned_test_BIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''names =  []\n",
    "for sent in ned_train_BIO:\n",
    "    for token, label in sent:\n",
    "        if label == 'B-PER':\n",
    "            names.append(token)\n",
    "r = Counter(names)\n",
    "print(r.keys())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Train Classifier**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nltk.tag.CRFTagger()\n",
    "model.train(ned_train_BIO, 'models/ned_baseline_BIO.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ned_BIO = model.tag_sents(X_test_BIO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:\n",
      "8.391 O      PUNCTUATION\n",
      "4.917 O      SUF_n\n",
      "4.892 O      SUF_t\n",
      "4.850 O      SUF_e\n",
      "4.548 O      SUF_E\n",
      "4.544 O      SUF_k\n",
      "4.496 O      WORD_U\n",
      "4.492 O      SUF_p\n",
      "4.450 O      SUF_m\n",
      "4.441 O      SUF_f\n",
      "4.416 O      SUF_g\n",
      "4.284 O      SUF_d\n",
      "4.256 O      SUF_s\n",
      "4.081 O      SUF_r\n",
      "4.034 O      HAS_NUM\n",
      "4.001 O      WORD_.\n",
      "3.968 O      SUF_l\n",
      "3.907 O      WORD_Ik\n",
      "3.872 O      WORD_Algemeen\n",
      "3.833 I-ORG  WORD_Morgen\n",
      "\n",
      "Top negative:\n",
      "-1.242 B-ORG  SUF_g\n",
      "-1.248 O      WORD_graf\n",
      "-1.256 O      WORD_eredienst\n",
      "-1.323 B-ORG  SUF_e\n",
      "-1.335 I-MISC SUF_i\n",
      "-1.338 O      SUF_ck\n",
      "-1.392 O      WORD_the\n",
      "-1.400 B-ORG  SUF_n\n",
      "-1.404 B-ORG  SUF_.\n",
      "-1.446 B-PER  SUF_w\n",
      "-1.462 O      SUF_adt\n",
      "-1.568 B-PER  HAS_NUM\n",
      "-1.576 I-ORG  WORD_De\n",
      "-1.592 B-ORG  SUF_k\n",
      "-1.592 O      SUF_our\n",
      "-1.643 B-LOC  HAS_NUM\n",
      "-1.749 O      WORD_der\n",
      "-1.788 O      WORD_den\n",
      "-2.936 O      WORD_&\n",
      "-6.022 O      CAPITALIZATION\n"
     ]
    }
   ],
   "source": [
    "info = model._tagger.info()\n",
    "\n",
    "def print_state_features(state_features):\n",
    "    for (attr, label), weight in state_features:\n",
    "        print(\"%0.3f %-6s %s\" % (weight, label, attr))    \n",
    "\n",
    "print(\"Top positive:\")\n",
    "print_state_features(Counter(info.state_features).most_common(20))\n",
    "\n",
    "print(\"\\nTop negative:\")\n",
    "print_state_features(Counter(info.state_features).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.75      0.69      0.72       774\n",
      "       I-LOC       0.44      0.37      0.40        49\n",
      "      B-MISC       0.80      0.60      0.68      1187\n",
      "      I-MISC       0.27      0.33      0.30       410\n",
      "       B-ORG       0.78      0.60      0.68       882\n",
      "       I-ORG       0.60      0.57      0.58       551\n",
      "       B-PER       0.67      0.73      0.70      1098\n",
      "       I-PER       0.76      0.91      0.83       807\n",
      "\n",
      "   micro avg       0.69      0.66      0.67      5758\n",
      "   macro avg       0.63      0.60      0.61      5758\n",
      "weighted avg       0.70      0.66      0.67      5758\n",
      " samples avg       0.05      0.05      0.05      5758\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred_BIO = [[word[1] for word in sent] for sent in pred_ned_BIO]\n",
    "print(bio_classification_report(y_test_BIO, y_pred_BIO))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 2452,\n",
       " 'incorrect': 715,\n",
       " 'partial': 48,\n",
       " 'missed': 711,\n",
       " 'spurious': 281,\n",
       " 'possible': 3926,\n",
       " 'actual': 3496,\n",
       " 'precision': 0.701,\n",
       " 'recall': 0.625,\n",
       " 'F1-score': 0.661}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, _ = compute_metrics(ned_test_BIO, pred_ned_BIO)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
