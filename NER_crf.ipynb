{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Courier New; color:#CCCCCC\">**Named Entity Recognition CRF**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Load Data and Imports**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\Jordi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import convert_BIO\n",
    "from NER_evaluation import *\n",
    "from feature_getter import Feature_getter\n",
    "import pycrfsuite\n",
    "\n",
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "esp_train = conll2002.iob_sents('esp.train') \n",
    "esp_val = conll2002.iob_sents('esp.testa')\n",
    "esp_test = conll2002.iob_sents('esp.testb')\n",
    "\n",
    "ned_train = conll2002.iob_sents('ned.train')\n",
    "ned_val = conll2002.iob_sents('ned.testa')\n",
    "ned_test = conll2002.iob_sents('ned.testb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Train Classifier**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_train = convert_BIO(esp_train)\n",
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter())\n",
    "model.train(esp_train, 'model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_test = convert_BIO(esp_test)\n",
    "X_esp_test = [[word[0] for word in sent] for sent in esp_test]\n",
    "pred = model.tag_sents(X_esp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'correct': 2577,\n",
       " 'incorrect': 530,\n",
       " 'partial': 108,\n",
       " 'missed': 393,\n",
       " 'spurious': 268,\n",
       " 'possible': 3608,\n",
       " 'actual': 3483,\n",
       " 'precision': 0.739879414298019,\n",
       " 'recall': 0.7142461197339246,\n",
       " 'F1-score': 0.726836835425187}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, results_agg_ent = compute_metrics(esp_test, pred)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Feature selection**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will try to perform the feature selection in order to achieve the best performance we can. We first contemplate the isolated effects of these features (only activating these)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = convert_BIO(esp_train)\n",
    "test_sents = convert_BIO(esp_test)\n",
    "val_sents = convert_BIO(esp_val)\n",
    "\n",
    "X_val_sents = [[word[0] for word in sent] for sent in val_sents]\n",
    "X_test_sents = [[word[0] for word in sent] for sent in test_sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by finding how the introduction of bigrams and trigrams can affect the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nltk.tag.CRFTagger()\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res_uni = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810721454717203"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_uni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(bigram = True, trigram = False, morphology = False, length = False, prefix = False,\n",
    "                 sufix = True, lemma = False, POS = False, shape = False))\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res_bi = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6605460832240371"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trigram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(bigram = True, trigram = True, morphology = False, length = False, prefix = False,\n",
    "                 sufix = True, lemma = False, POS = False, shape = False))\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res_tri = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6542188607894115"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the performance has rather decreased introducing these features. It could be due to the extra dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Without morphology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6810721454717203"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_uni "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Including morphology**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(bigram = False, trigram = False, morphology = True, length = False, prefix = False,\n",
    "                 sufix = True, lemma = False, POS = False, shape = False))\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "best_morphology = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6142925890279114"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_morphology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somehow we can see a drastic performance decrease when introducing morphology alone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including all other variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will continue with unigrams, but try to see the performance when incloduing all variables of the token we are analising at the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(bigram = False, trigram = False, morphology = True, length = True, prefix = True,\n",
    "                 sufix = True, lemma = True, POS = True, shape = True))\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "best_other = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6606003308910423"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interactions(all)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try to introduce bigrams and trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter())\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "best_all = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.682744960969358"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get more or less the same performance as we get without the custom feature extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Hiperparameters selection**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin with hiperparameters selection. However, we will perform it on the base features of the classifier. The reason lies in the runtime that a training with all features bears with it, along with the assumption that the hiperparameters doesnt have distinct interactions among the different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try to do a custom function that does a gridsearch over the different values we try to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'c1': [0.1, 0.5, 1.0],\n",
    "    'c2': [0.1, 0.5, 1.0],\n",
    "    'max_iterations': [50, 100, 200]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_cv(hyperparameters,train_sents,val_sents,X_val_sents):\n",
    "    results_df = pd.DataFrame(columns = ['c1', 'c2', 'max_iterations', 'F1-score'])\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    num_combinations = len(hyperparameters['c1']) * len(hyperparameters['c2']) * len(hyperparameters['max_iterations'])\n",
    "    current_combination = 0\n",
    "    for c1 in hyperparameters['c1']:\n",
    "        for c2 in hyperparameters['c2']:\n",
    "            for max_iter in hyperparameters['max_iterations']:\n",
    "                current_combination += 1\n",
    "                print(f'Fitting model {current_combination} of {num_combinations}', end = '\\r')\n",
    "                model = nltk.tag.CRFTagger(training_opt = {'c1': c1, 'c2': c2, 'max_iterations': max_iter})\n",
    "                model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "                pred = model.tag_sents(X_val_sents)\n",
    "                results, _ = compute_metrics(val_sents, pred)\n",
    "                results_df.loc[len(results_df)] = [c1, c2, max_iter, results['F1-score']]\n",
    "                if results['F1-score'] > best_f1:\n",
    "                    best_f1 = results['F1-score']\n",
    "                    best_params = {'c1': c1, 'c2': c2, 'max_iterations': max_iter}\n",
    "\n",
    "    return best_f1,best_params,results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 27 of 27\r"
     ]
    }
   ],
   "source": [
    "best, best_params,dataframe = gridsearch_cv(hyperparameters,train_sents,val_sents,X_val_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>max_iterations</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.706310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.704532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.695958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.694897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.693380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.690627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.689566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.688490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.684173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.680696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     c1   c2  max_iterations  F1-score\n",
       "1   0.1  0.1           100.0  0.706310\n",
       "2   0.1  0.1           200.0  0.704532\n",
       "0   0.1  0.1            50.0  0.695958\n",
       "10  0.5  0.1           100.0  0.694897\n",
       "11  0.5  0.1           200.0  0.693380\n",
       "9   0.5  0.1            50.0  0.690627\n",
       "5   0.1  0.5           200.0  0.689566\n",
       "4   0.1  0.5           100.0  0.688490\n",
       "3   0.1  0.5            50.0  0.684173\n",
       "14  0.5  0.5           200.0  0.680696"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sort_values(by = 'F1-score', ascending = False).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now lets try with the complete model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch_cv_complete(hyperparameters,train_sents,val_sents,X_val_sents):\n",
    "    results_df = pd.DataFrame(columns = ['c1', 'c2', 'max_iterations', 'F1-score'])\n",
    "    best_f1 = 0\n",
    "    best_params = {}\n",
    "    num_combinations = len(hyperparameters['c1']) * len(hyperparameters['c2']) * len(hyperparameters['max_iterations'])\n",
    "    current_combination = 0\n",
    "    for c1 in hyperparameters['c1']:\n",
    "        for c2 in hyperparameters['c2']:\n",
    "            for max_iter in hyperparameters['max_iterations']:\n",
    "                current_combination += 1\n",
    "                print(f'Fitting model {current_combination} of {num_combinations}', end = '\\r')\n",
    "                model = nltk.tag.CRFTagger(training_opt = {'c1': c1, 'c2': c2, 'max_iterations': max_iter}, feature_func = Feature_getter())\n",
    "                model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "                pred = model.tag_sents(X_val_sents)\n",
    "                results, _ = compute_metrics(val_sents, pred)\n",
    "                results_df.loc[len(results_df)] = [c1, c2, max_iter, results['F1-score']]\n",
    "                if results['F1-score'] > best_f1:\n",
    "                    best_f1 = results['F1-score']\n",
    "                    best_params = {'c1': c1, 'c2': c2, 'max_iterations': max_iter}\n",
    "\n",
    "    return best_f1,best_params,results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "best_complete, best_params_complete,dataframe_complete = gridsearch_cv_complete(hyperparameters,train_sents,val_sents,X_val_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>max_iterations</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.680509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.680135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.677962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.677239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.676632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.675064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.673350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.672271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.671928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>0.671798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     c1   c2  max_iterations  F1-score\n",
       "8   0.1  1.0           200.0  0.680509\n",
       "7   0.1  1.0           100.0  0.680135\n",
       "17  0.5  1.0           200.0  0.677962\n",
       "5   0.1  0.5           200.0  0.677239\n",
       "14  0.5  0.5           200.0  0.676632\n",
       "13  0.5  0.5           100.0  0.675064\n",
       "4   0.1  0.5           100.0  0.673350\n",
       "6   0.1  1.0            50.0  0.672271\n",
       "2   0.1  0.1           200.0  0.671928\n",
       "11  0.5  0.1           200.0  0.671798"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_complete.sort_values(by = 'F1-score', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters_default = {'c1':0.1, 'c2':0.1, 'max_iterations':100}\n",
    "hyperparameters_custom = {'c1':0.1, 'c2':1, 'max_iterations':100}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to play with a parameter that can help the score of custom features addition:             \n",
    "- 'feature.minfreq': The minimum frequency of features.\n",
    "- 'feature.possible_states': Force to generate possible state features.\n",
    "-  'feature.possible_transitions': Force to generate possible transition features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(), training_opt = {\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2':1.0,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    # include transitions that are possible, but not observed\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6805830903790087"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improve only a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now with feature possible states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(), training_opt = {\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2':1.0,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    \n",
    "    'feature.possible_states': True\n",
    "})\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.684733091928513"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a little more improvement here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets try to establish minfreq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(), training_opt = {\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2':1.0,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "\n",
    "    \n",
    "    'feature.minfreq': 5\n",
    "})\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.659353483486988"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 10238...\r"
     ]
    }
   ],
   "source": [
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(), training_opt = {\n",
    "    'c1': 0.1,   # coefficient for L1 penalty\n",
    "    'c2':1.0,  # coefficient for L2 penalty\n",
    "    'max_iterations': 100,  # stop earlier\n",
    "    'feature.possible_states': True,\n",
    "    'feature.possible_transitions': True\n",
    "})\n",
    "model.train(train_sents, 'model.crf.tagger')\n",
    "\n",
    "pred = model.tag_sents(X_val_sents)\n",
    "results, _ = compute_metrics(val_sents, pred)\n",
    "\n",
    "res = results['F1-score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.683313885647608"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot see much improvement with the introduction of these two variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
