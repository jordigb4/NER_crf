{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:Courier New; color:#CCCCCC\">**Named Entity Recognition CRF**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Load Data and Imports**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package conll2002 to\n",
      "[nltk_data]     C:\\Users\\jerez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2002 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import convert_BIO\n",
    "from NER_evaluation import *\n",
    "from feature_getter import Feature_getter\n",
    "import pycrfsuite\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "nltk.download('conll2002')\n",
    "from nltk.corpus import conll2002\n",
    "\n",
    "#Dutch Data\n",
    "ned_train = conll2002.iob_sents('ned.train')\n",
    "ned_test = conll2002.iob_sents('ned.testb')\n",
    "\n",
    "#Spanish Data\n",
    "esp_train = conll2002.iob_sents('esp.train')\n",
    "esp_test = conll2002.iob_sents('esp.testb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Preprocessing Data**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dutch\n",
    "ned_train_BIO = convert_BIO(ned_train)\n",
    "ned_test_BIO = convert_BIO(ned_test)\n",
    "\n",
    "X_ned_test_BIO = [[word[0] for word in sent] for sent in ned_test_BIO]\n",
    "y_ned_test_BIO = [[word[1] for word in sent] for sent in ned_test_BIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spanish\n",
    "esp_train_BIO = convert_BIO(esp_train)\n",
    "esp_test_BIO = convert_BIO(esp_test)\n",
    "\n",
    "X_esp_test_BIO = [[word[0] for word in sent] for sent in esp_test_BIO]\n",
    "y_esp_test_BIO = [[word[1] for word in sent] for sent in esp_test_BIO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Train Classifier**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summary avaluation tables\n",
    "results_esp = pd.DataFrame()\n",
    "results_ned = pd.DataFrame()\n",
    "def save_ent_results(nclf, results, results_agg_ent, df):\n",
    "    df.loc[nclf,'total acc'] = results[\"precision\"]\n",
    "    df.loc[nclf,'total recall'] = results[\"recall\"]\n",
    "    df.loc[nclf,'total F1'] = results[\"F1-score\"]\n",
    "    df.loc[nclf,'PER F1'] = results_agg_ent[\"PER\"][\"F1-score\"]\n",
    "    df.loc[nclf,'ORG F1'] = results_agg_ent[\"ORG\"][\"F1-score\"]\n",
    "    df.loc[nclf,'LOC F1'] = results_agg_ent[\"LOC\"][\"F1-score\"]\n",
    "    df.loc[nclf,'MISC F1'] = results_agg_ent[\"MISC\"][\"F1-score\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:Courier New; color:#336633\">**Dutch Classifier**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Courier New\">Hyper&feature_opt-ned notebook suggests a model with our customed Feature Getter (not considering Next token features), for which best hyperparamaters are: {'c1': 0.01, 'c2': 0.1, 'max_iterations': 200, 'possible_transitions': True, 'possible_states': True, 'min_freq' = 0}. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hyperparams = {'c1': 0.01, 'c2': 0.1, 'max_iterations': 50, 'feature.possible_transitions': True,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(language='ned', next_tok=False), training_opt = default_hyperparams)\n",
    "model.train(ned_train_BIO, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**Evalutation**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.87      0.81      0.84       774\n",
      "       I-LOC       0.67      0.53      0.59        49\n",
      "      B-MISC       0.87      0.76      0.81      1187\n",
      "      I-MISC       0.63      0.46      0.53       410\n",
      "       B-ORG       0.81      0.71      0.76       882\n",
      "       I-ORG       0.80      0.65      0.72       551\n",
      "       B-PER       0.78      0.90      0.83      1098\n",
      "       I-PER       0.87      0.96      0.91       807\n",
      "\n",
      "   micro avg       0.82      0.78      0.80      5758\n",
      "   macro avg       0.79      0.72      0.75      5758\n",
      "weighted avg       0.82      0.78      0.79      5758\n",
      " samples avg       0.07      0.07      0.07      5758\n",
      "\n",
      "================================================================================\n",
      "Entity level evaluation\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total acc</th>\n",
       "      <th>total recall</th>\n",
       "      <th>total F1</th>\n",
       "      <th>PER F1</th>\n",
       "      <th>ORG F1</th>\n",
       "      <th>LOC F1</th>\n",
       "      <th>MISC F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dutch_BIO</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           total acc  total recall  total F1  PER F1  ORG F1  LOC F1  MISC F1\n",
       "Dutch_BIO       0.81         0.786     0.798   0.774   0.762   0.866    0.807"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ned_BIO = model.tag_sents(X_ned_test_BIO)\n",
    "y_pred_BIO = [[word[1] for word in sent] for sent in pred_ned_BIO]\n",
    "\n",
    "print(bio_classification_report(y_ned_test_BIO, y_pred_BIO))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(ned_test_BIO, pred_ned_BIO)\n",
    "results_ned = save_ent_results(\"Dutch_BIO\", results, results_agg_ent, results_ned)\n",
    "results_ned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**Feature Importance**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top positive:                       |     Top negative:\n",
      "-----------------------------------------------------------------------------\n",
      "5.861 O      PUNCTUATION                -1.943 O      WORD_Gent                 \n",
      "5.023 O      SHAPE_xxxx                 -2.064 O      SUF_our                   \n",
      "3.713 B-LOC  WORD_Gent                  -2.158 B-PER  SHAPE_xxxx                \n",
      "3.346 B-MISC SHAPE_XXX-xxxx             -2.177 O      LEMMA_groenen             \n",
      "3.298 O      WORD_.                     -2.288 B-PER  HAS_NUM                   \n",
      "3.274 O      SHAPE_xxxx-xxxx            -2.395 O      -1_SHAPE_xxx              \n",
      "3.091 O      POS_PUNCT                  -2.532 B-MISC POS_INTJ                  \n",
      "2.977 O      HAS_NUM                    -2.537 I-MISC -1_SUF_se                 \n",
      "2.622 O      SHAPE_XXXX                 -2.590 B-LOC  SHAPE_xxxx                \n",
      "2.552 O      POS_ADV                    -2.755 B-ORG  SHAPE_xxxx                \n"
     ]
    }
   ],
   "source": [
    "def print_state_features(state_features):\n",
    "        for (attr, label), weight in state_features:\n",
    "            string = \"%0.3f %-6s %s\" % (weight, label, attr)\n",
    "            print(string, end = \" \"*(40 - len(string)))\n",
    "\n",
    "def feature_importance(model):\n",
    "\n",
    "    info = model._tagger.info()\n",
    "    positive_features = Counter(info.state_features).most_common(10)\n",
    "    negative_features = Counter(info.state_features).most_common()[-10:]\n",
    "\n",
    "    print(\"Top positive:                       |     Top negative:\")\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "\n",
    "    for positive, negative in zip(positive_features, negative_features):\n",
    "        print_state_features([positive])\n",
    "        print_state_features([negative])\n",
    "        print()\n",
    "feature_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:Courier New; color:#336633\">**Spanish Classifier**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Courier New\">Hyper&feature_opt-esp notebook suggests a model ... which best hyperparamaters are: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "customed_hyperparams = {'c1': 0.01, 'c2': 1, 'max_iterations': 100, 'feature.possible_transitions': False,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func=Feature_getter(), training_opt = customed_hyperparams)\n",
    "model.train(esp_train_BIO, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**Evalutation**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_esp_BIO = model.tag_sents(X_esp_test_BIO)\n",
    "y_pred_BIO = [[word[1] for word in sent] for sent in pred_esp_BIO]\n",
    "\n",
    "print(bio_classification_report(y_esp_test_BIO, y_pred_BIO))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(esp_test_BIO, pred_esp_BIO)\n",
    "results_esp = save_ent_results(\"Spanish_BIO\", results, results_agg_ent, results_esp)\n",
    "results_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**Feature Importance**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>See:</b> ... (comparison between spanish and dutch)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Changing Tagger Format**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Courier New\">At this point, lets check whether changing the codification of entities has a postive impact on performance. </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:Courier New; color:#336633\">**Dutch Classifier**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**IO**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_train_IO = convert_BIO(ned_train, begin = False)\n",
    "ned_test_IO = convert_BIO(ned_test, begin = False)\n",
    "\n",
    "X_ned_test_IO = [[word[0] for word in sent] for sent in ned_test_IO]\n",
    "y_ned_test_IO = [[word[1] for word in sent] for sent in ned_test_IO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hyperparams = {'c1': 0.01, 'c2': 0.1, 'max_iterations': 50, 'feature.possible_transitions': True,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(language='ned', next_tok=False), training_opt = default_hyperparams)\n",
    "model.train(ned_train_IO, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-LOC       0.80      0.81      0.80       823\n",
      "      I-MISC       0.77      0.66      0.71      1597\n",
      "       I-ORG       0.81      0.66      0.73      1433\n",
      "       I-PER       0.82      0.92      0.87      1905\n",
      "\n",
      "   micro avg       0.80      0.77      0.78      5758\n",
      "   macro avg       0.80      0.76      0.78      5758\n",
      "weighted avg       0.80      0.77      0.78      5758\n",
      " samples avg       0.06      0.06      0.06      5758\n",
      "\n",
      "================================================================================\n",
      "Entity level evaluation\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total acc</th>\n",
       "      <th>total recall</th>\n",
       "      <th>total F1</th>\n",
       "      <th>PER F1</th>\n",
       "      <th>ORG F1</th>\n",
       "      <th>LOC F1</th>\n",
       "      <th>MISC F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dutch_BIO</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch_IO</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           total acc  total recall  total F1  PER F1  ORG F1  LOC F1  MISC F1\n",
       "Dutch_BIO      0.810         0.786     0.798   0.774   0.762   0.866    0.807\n",
       "Dutch_IO       0.784         0.750     0.766   0.764   0.727   0.830    0.752"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ned_IO = model.tag_sents(X_ned_test_IO)\n",
    "y_pred_IO = [[word[1] for word in sent] for sent in pred_ned_IO]\n",
    "\n",
    "print(bio_classification_report(y_ned_test_IO, y_pred_IO))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(ned_test_IO, pred_ned_IO)\n",
    "results_ned = save_ent_results(\"Dutch_IO\", results, results_agg_ent, results_ned)\n",
    "results_ned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**BIOS**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_train_BIOS = convert_BIO(ned_train, begin = True, single = True)\n",
    "ned_test_BIOS = convert_BIO(ned_test, begin = True, single = True)\n",
    "\n",
    "X_ned_test_BIOS = [[word[0] for word in sent] for sent in ned_test_BIOS]\n",
    "y_ned_test_BIOS = [[word[1] for word in sent] for sent in ned_test_BIOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hyperparams = {'c1': 0.01, 'c2': 0.1, 'max_iterations': 50, 'feature.possible_transitions': True,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(language='ned', next_tok=False), training_opt = default_hyperparams)\n",
    "model.train(ned_train_BIOS, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.68      0.43      0.53        60\n",
      "       I-LOC       0.67      0.53      0.59        49\n",
      "       S-LOC       0.89      0.85      0.87       714\n",
      "      B-MISC       0.75      0.52      0.61       372\n",
      "      I-MISC       0.61      0.45      0.52       410\n",
      "      S-MISC       0.86      0.81      0.84       815\n",
      "       B-ORG       0.79      0.70      0.74       430\n",
      "       I-ORG       0.80      0.63      0.71       551\n",
      "       S-ORG       0.75      0.67      0.71       452\n",
      "       B-PER       0.85      0.93      0.89       708\n",
      "       I-PER       0.86      0.96      0.91       807\n",
      "       S-PER       0.65      0.81      0.72       390\n",
      "\n",
      "   micro avg       0.80      0.77      0.78      5758\n",
      "   macro avg       0.76      0.69      0.72      5758\n",
      "weighted avg       0.80      0.77      0.78      5758\n",
      " samples avg       0.06      0.06      0.06      5758\n",
      "\n",
      "================================================================================\n",
      "Entity level evaluation\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total acc</th>\n",
       "      <th>total recall</th>\n",
       "      <th>total F1</th>\n",
       "      <th>PER F1</th>\n",
       "      <th>ORG F1</th>\n",
       "      <th>LOC F1</th>\n",
       "      <th>MISC F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dutch_BIO</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch_IO</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch_BIOS</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total acc  total recall  total F1  PER F1  ORG F1  LOC F1  MISC F1\n",
       "Dutch_BIO       0.810         0.786     0.798   0.774   0.762   0.866    0.807\n",
       "Dutch_IO        0.784         0.750     0.766   0.764   0.727   0.830    0.752\n",
       "Dutch_BIOS      0.807         0.781     0.794   0.772   0.747   0.876    0.800"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ned_BIOS = model.tag_sents(X_ned_test_BIOS)\n",
    "y_pred_BIOS = [[word[1] for word in sent] for sent in pred_ned_BIOS]\n",
    "\n",
    "print(bio_classification_report(y_ned_test_BIOS, y_pred_BIOS))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(ned_test_BIOS, pred_ned_BIOS)\n",
    "results_ned = save_ent_results(\"Dutch_BIOS\", results, results_agg_ent, results_ned)\n",
    "results_ned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**BIOES**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ned_train_BIOES = convert_BIO(ned_train, begin = True, single = True, end = True)\n",
    "ned_test_BIOES = convert_BIO(ned_test, begin = True, single = True, end = True)\n",
    "\n",
    "X_ned_test_BIOES = [[word[0] for word in sent] for sent in ned_test_BIOES]\n",
    "y_ned_test_BIOES = [[word[1] for word in sent] for sent in ned_test_BIOES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hyperparams = {'c1': 0.01, 'c2': 0.1, 'max_iterations': 50, 'feature.possible_transitions': True,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func = Feature_getter(language='ned', next_tok=False), training_opt = default_hyperparams)\n",
    "model.train(ned_train_BIOES, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.69      0.45      0.55        60\n",
      "       E-LOC       0.69      0.57      0.62        42\n",
      "       I-LOC       0.33      0.14      0.20         7\n",
      "       S-LOC       0.88      0.86      0.87       714\n",
      "      B-MISC       0.74      0.51      0.60       372\n",
      "      E-MISC       0.68      0.47      0.55       260\n",
      "      I-MISC       0.46      0.35      0.39       150\n",
      "      S-MISC       0.87      0.81      0.84       815\n",
      "       B-ORG       0.78      0.70      0.74       430\n",
      "       E-ORG       0.80      0.73      0.76       399\n",
      "       I-ORG       0.62      0.30      0.40       152\n",
      "       S-ORG       0.74      0.68      0.71       452\n",
      "       B-PER       0.86      0.94      0.90       708\n",
      "       E-PER       0.86      0.96      0.91       690\n",
      "       I-PER       0.87      0.89      0.88       117\n",
      "       S-PER       0.66      0.81      0.72       390\n",
      "\n",
      "   micro avg       0.80      0.76      0.78      5758\n",
      "   macro avg       0.72      0.64      0.67      5758\n",
      "weighted avg       0.80      0.76      0.77      5758\n",
      " samples avg       0.06      0.06      0.06      5758\n",
      "\n",
      "================================================================================\n",
      "Entity level evaluation\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total acc</th>\n",
       "      <th>total recall</th>\n",
       "      <th>total F1</th>\n",
       "      <th>PER F1</th>\n",
       "      <th>ORG F1</th>\n",
       "      <th>LOC F1</th>\n",
       "      <th>MISC F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dutch_BIO</th>\n",
       "      <td>0.810</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.774</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch_IO</th>\n",
       "      <td>0.784</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.727</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch_BIOS</th>\n",
       "      <td>0.807</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.876</td>\n",
       "      <td>0.800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dutch_BIOES</th>\n",
       "      <td>0.809</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             total acc  total recall  total F1  PER F1  ORG F1  LOC F1  \\\n",
       "Dutch_BIO        0.810         0.786     0.798   0.774   0.762   0.866   \n",
       "Dutch_IO         0.784         0.750     0.766   0.764   0.727   0.830   \n",
       "Dutch_BIOS       0.807         0.781     0.794   0.772   0.747   0.876   \n",
       "Dutch_BIOES      0.809         0.784     0.796   0.782   0.742   0.874   \n",
       "\n",
       "             MISC F1  \n",
       "Dutch_BIO      0.807  \n",
       "Dutch_IO       0.752  \n",
       "Dutch_BIOS     0.800  \n",
       "Dutch_BIOES    0.802  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ned_BIOES = model.tag_sents(X_ned_test_BIOES)\n",
    "y_pred_BIOES = [[word[1] for word in sent] for sent in pred_ned_BIOES]\n",
    "\n",
    "print(bio_classification_report(y_ned_test_BIOES, y_pred_BIOES))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(ned_test_BIOES, pred_ned_BIOES)\n",
    "results_ned = save_ent_results(\"Dutch_BIOES\", results, results_agg_ent, results_ned)\n",
    "results_ned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Courier New\">As we can see, the codification that works best is BIO, with the higher F1-score. On the other side, we find that IO is the worst, surely because of its lack of information. What is sure is that adding 'Single' label turns out in good models. This points us that there are plenty of single token entities. Further actions could be taken with this type of models, but we will continue in the same line, with the BIO model.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:Courier New; color:#336633\">**Spanish Classifier**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**IO**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_train_IO = convert_BIO(esp_train, begin = False)\n",
    "esp_test_IO = convert_BIO(esp_test, begin = False)\n",
    "\n",
    "X_esp_test_IO = [[word[0] for word in sent] for sent in esp_test_IO]\n",
    "y_esp_test_IO = [[word[1] for word in sent] for sent in esp_test_IO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customed_hyperparams = {'c1': 0.01, 'c2': 1, 'max_iterations': 100, 'feature.possible_transitions': False,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func=Feature_getter(), training_opt = customed_hyperparams)\n",
    "model.train(esp_train_IO, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_esp_IO = model.tag_sents(X_esp_test_IO)\n",
    "y_pred_IO = [[word[1] for word in sent] for sent in pred_esp_IO]\n",
    "\n",
    "print(bio_classification_report(y_esp_test_IO, y_pred_IO))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(esp_test_IO, pred_esp_IO)\n",
    "results_esp = save_ent_results(\"Spanish_IO\", results, results_agg_ent, results_esp)\n",
    "results_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**BIOS**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_train_BIOS = convert_BIO(esp_train, begin = True, single = True)\n",
    "esp_test_BIOS = convert_BIO(esp_test, begin = True, single = True)\n",
    "\n",
    "X_esp_test_BIOS = [[word[0] for word in sent] for sent in esp_test_BIOS]\n",
    "y_esp_test_BIOS = [[word[1] for word in sent] for sent in esp_test_BIOS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customed_hyperparams = {'c1': 0.01, 'c2': 1, 'max_iterations': 100, 'feature.possible_transitions': False,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func=Feature_getter(), training_opt = customed_hyperparams)\n",
    "model.train(esp_train_BIOS, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_esp_BIOS = model.tag_sents(X_esp_test_BIOS)\n",
    "y_pred_BIOS = [[word[1] for word in sent] for sent in pred_esp_BIOS]\n",
    "\n",
    "print(bio_classification_report(y_esp_test_BIOS, y_pred_BIOS))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(esp_test_BIOS, pred_esp_BIOS)\n",
    "results_esp = save_ent_results(\"Spanish_BIOS\", results, results_agg_ent, results_esp)\n",
    "results_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"font-family:Courier New; color:#994C00\">**BIOES**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "esp_train_BIOES = convert_BIO(esp_train, begin = True, single = True, end = True)\n",
    "esp_test_BIOES = convert_BIO(esp_test, begin = True, single = True, end = True)\n",
    "\n",
    "X_esp_test_BIOES = [[word[0] for word in sent] for sent in esp_test_BIOES]\n",
    "y_esp_test_BIOES = [[word[1] for word in sent] for sent in esp_test_BIOES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customed_hyperparams = {'c1': 0.01, 'c2': 1, 'max_iterations': 100, 'feature.possible_transitions': False,\n",
    "                                            'feature.possible_states': True, 'feature.minfreq': 0}\n",
    "model = nltk.tag.CRFTagger(feature_func=Feature_getter(), training_opt = customed_hyperparams)\n",
    "model.train(esp_train_BIOES, 'models/model.crf.tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_esp_BIOES = model.tag_sents(X_esp_test_BIOES)\n",
    "y_pred_BIOES = [[word[1] for word in sent] for sent in pred_esp_BIOES]\n",
    "\n",
    "print(bio_classification_report(y_esp_test_BIOES, y_pred_BIOES))\n",
    "print('='*80)\n",
    "print('Entity level evaluation')\n",
    "print('='*80)\n",
    "results, results_agg_ent = compute_metrics(esp_test_BIOES, pred_esp_BIOES)\n",
    "results_esp = save_ent_results(\"Spanish_BIOES\", results, results_agg_ent, results_esp)\n",
    "results_esp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Adding Gazetteers**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''names =  []\n",
    "for sent in ned_train_BIO:\n",
    "    for token, label in sent:\n",
    "        if label == 'B-PER':\n",
    "            names.append(token)\n",
    "r = Counter(names)\n",
    "print(r.keys())'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:Courier New; color:#336666\">**Language Models Comparsion**</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-family:Courier New\">In this section, once we have found the best models for each language, we consider opportune to make a comparison.</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
